% !TEX encoding = UTF-8 Unicode

\chapter{Introduzione}
L'informatica è il settore tecnologico che si evolve con maggiore velocità, basti pensare che la Prima legge di Moore\footnote{La prima legge di Moore è un'osservazione empirica di Gordon Moore, cofondatore di \emph{Fairchild Semiconductor} e di \emph{Intel}.}, asserisce che la potenza dei calcolatori raddoppia ogni diciotto mesi. Una simile progressione in un settore tecnologico assai più anziano come l'aeronautica, permetterebbe oggi di fare un giro completo della terra in una manciata di secondi. Questa potenza di calcolo permette lo sviluppo di applicazioni impensabili fino ad una decina di anni fa, capaci di trattare un insieme di dati da elaborare estremamente grande. Un esempio significativo è l'ammontare di dati che viene generato dai sensori dell'acceleratore di particelle del CERN di Ginevra, secondo \cite{pres_cern} circa 15 PetaByte l'anno, i quali devono essere memorizzati e poi elaborati. Il web tutt'oggi genera dati analoghi, ed ogni giorno vengono memorizzati diversi terabyte di dati a seguito di ricerche, acquisti, pubblicazione di contenuti sui social network, eccetera. 
\begin{figure}
	\centering
	\includegraphics[width=0.50\textwidth]{Data-in-One-Minute.jpg}
	\label{data_per_minute}
	\caption{Quantità di dati al minuto pero alcuni siti in rete.}
\end{figure}
L'infografica nella figura \ref{data_per_minute} tratta da \cite{data_per_minute} ci permette di vedere con immediatezza di quali quantità stiamo parlando, e possiamo evincere che per una tale mole di informazioni occorrono delle procedure di trattamento dell'informazione diverse da quelle che possono essere adottate da un programma per gestire una semplice rubrica. Stiamo parlando quindi di diversi GigaByte se non TeraByte di dati, che hanno bisogno di essere analizzati e processati in tempo modesto. I tipi di elaborazione necessari possono essere il calcolo di statistiche, una suddivisione per categoria, un'interrogazione della base di dati, oppure una serie di calcoli che possono permettere ad un'azienda di capire, in base agli acquisti dei suoi clienti, i trend riguardo alcuni suoi prodotti. Le tecniche elaborate nel corso degli anni hanno portato alla nascita di una nuova branca dell'informatica, quella dei \emph{Big Data}. Secondo \cite{rezzani2013big} i \emph{Big Data}
\begin{quotation}
sono dati che superano i limiti degli strumenti di database tradizionali.
\end{quotation}
Con questo termine però non si intendono solo i dataset di grandi dimensioni, ma anche le tecniche utilizzate per elaborarli ed estrarre informazioni significative da esso. L'\emph{obbiettivo} di queste tecniche è quindi quello di estrarre \emph{informazioni significative}, per le quali intendiamo qualsiasi tipo di statistica che possa essere calcolata solamente prendendo in considerazione tutti i dati esistenti, impossibile da calcolare a mano o farsene un'idea "guardando" i dati. Tornando alla figura \ref{data_per_minute}, il risultato dell'elaborazione dei dati è stato presentato in modo da capirlo in modo immediato, dando la possibilità al lettore di trarre immediatamente le proprie conclusioni.
Si può suddividere il processo che vedono entrare in gioco i big data in tre parti:
\begin{enumerate}
\item Raccolta dei dati
\item Elaborazione
\item Visualizzazione.
\end{enumerate}
La raccolta dei dati è la prima parte del processo, e tratta la memorizzazione dei dati su database o tramite altre tecnologie. Spesso però questa fase è effettuata per altri scopi piuttosto che per effettuare delle elaborazioni tramite gli strumenti dei Big Data. Questo può comportare un dataset formato da dati provenienti da fonti diverse, che utilizzano tecnologie e standard diversi che devono essere aggregati, inoltre non tutte le tuple potrebbero essere complete, con campi mancanti, dati non consistenti eccetera. Anche questi problemi dovranno essere affrontati, i Big Data dopotutto non sono stati definiti come \emph{dati omogenei}.
La parte di elaborazione prevede una serie di tecniche atte ad analizzare tutti i dati in un tempo accettabile, ed effettuare i calcoli. Questa parte prevede anche tutti quegli algoritmi statistici utili ad estrarre informazioni significative da campioni di dati.
La parte di visualizzazione si occupa di presentare i risultati in modo che sia possibile interpretarli immediatamente e farsi un'idea delle proprietà emerse dall'elaborazione rispetto alla totalità dei dati.
In questo testo verranno accennate le prime due parti, mentre per terza parte si presenteranno strumenti di programmazione utili allo scopo, per essere poi applicati al progetto \emph{Piedmont Heritage}.